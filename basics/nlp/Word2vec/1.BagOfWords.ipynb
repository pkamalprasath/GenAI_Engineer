{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713b2452",
   "metadata": {},
   "source": [
    "### BagofWords\n",
    "The Bag-of-Words (BoW) model is a fundamental technique in Natural Language Processing (NLP) that is used to prepare text data for machine learning algorithms. Its core purpose is to convert unstructured text into a fixed-length numerical vector, which a machine learning model can easily process.It is called a \"bag\" of words because it treats a piece of text (like a document or a sentence) as an unordered collection of its words, completely disregarding grammar and word order.\n",
    "\n",
    "1. Build the Vocabulary\n",
    "2. Create the Document Vectors (Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa2c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to import the file with sample data\n",
    "# use sep \\t to seperate the input data into lables and message \n",
    "# if pandas libraries are not present pip install that. \n",
    "\n",
    "import pandas as pd\n",
    "messages=pd.read_csv('D:/AI/KrishNaik_Academy/Coding/NLP/Text_Prepocessing/input/SMSSpamCollection',\n",
    "                     sep='\\t',names=[\"lables\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187bf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a878e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Data cleaning and Preprocessing\n",
    "import re # importing regular expression lib\n",
    "import nltk # for text preprocessing \n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8526d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesinng -> \n",
    "#   ->Match the regex ,replace non alphabet with space \n",
    "#   ->Stemming is applied\n",
    "#   ->convert that into lower case\n",
    "#   ->split the final word \n",
    "#   ->Join them into document and store is in list named corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "ps=PorterStemmer()\n",
    "corpus=[] # empty list \n",
    "for i in range(0,len(messages)):\n",
    "    #From re lib we are using \"sub\" function to match the regex \n",
    "    #replace that other character with space and store it in messages list \n",
    "    review=re.sub('[^a-zA-Z]',' ',messages['message'][i])  \n",
    "    review=review.lower()\n",
    "    review=review.split() \n",
    "    review=[ps.stem(word) for word in review if word not in set(stopwords.words('english')) ] \n",
    "    review=' '.join(review)  \n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "   ## Create a BOW \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#we are picking only 100 words which has high frequency\n",
    "cv=CountVectorizer(max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d91dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c45f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b11fa236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(5572, 100))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1085ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
