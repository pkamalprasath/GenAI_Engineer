{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713b2452",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "N-grams are a simple but powerful concept in Natural Language Processing (NLP) and text analysis. They are contiguous sequences of $N$ items (words, letters, or sometimes characters) from a given sample of text.They are used primarily to capture local context and address the major limitation of the Bag-of-Words model, which completely ignores word order.\n",
    "Unigram (N=1);Bigram (N=2);Trigram (N=3) ;ngram_range=(min_n, max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa2c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to import the file with sample data\n",
    "# use sep \\t to seperate the input data into lables and message \n",
    "# if pandas libraries are not present pip install that. \n",
    "\n",
    "import pandas as pd\n",
    "messages=pd.read_csv('D:/AI/KrishNaik_Academy/Coding/NLP/Text_Prepocessing/input/SMSSpamCollection',\n",
    "                     sep='\\t',names=[\"lables\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187bf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a878e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Data cleaning and Preprocessing\n",
    "import re # importing regular expression lib\n",
    "import nltk # for text preprocessing \n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8526d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesinng -> \n",
    "#   ->Match the regex ,replace non alphabet with space \n",
    "#    ->Stemming is applied\n",
    "#   ->convert that into lower case\n",
    "#   ->split the final word \n",
    "#   ->Join them into document and store is in list named corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "ps=PorterStemmer()\n",
    "corpus=[] # empty list \n",
    "for i in range(0,len(messages)):\n",
    "    #From re lib we are using \"sub\" function to match the regex \n",
    "    #replace that other character with space and store it in messages list \n",
    "    review=re.sub('[^a-zA-Z]',' ',messages['message'][i])  \n",
    "    review=review.lower()\n",
    "    review=review.split() \n",
    "    review=[ps.stem(word) for word in review if word not in set(stopwords.words('english')) ] \n",
    "    review=' '.join(review)  \n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066d829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\ai\\krishnaik_academy\\venv1\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in d:\\ai\\krishnaik_academy\\venv1\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\ai\\krishnaik_academy\\venv1\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\ai\\krishnaik_academy\\venv1\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\ai\\krishnaik_academy\\venv1\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb6f23",
   "metadata": {},
   "source": [
    "## NGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a BOW \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#we are picking only 100 words which has high frequency\n",
    "cv=CountVectorizer(max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d91dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c45f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b11fa236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(5572, 100))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1085ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73aaef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'like lt gt': np.int64(43),\n",
       " 'sorri call later': np.int64(79),\n",
       " 'pleas call custom': np.int64(65),\n",
       " 'call custom servic': np.int64(6),\n",
       " 'custom servic repres': np.int64(22),\n",
       " 'guarante cash prize': np.int64(35),\n",
       " 'draw show prize': np.int64(23),\n",
       " 'show prize guarante': np.int64(77),\n",
       " 'prize guarante call': np.int64(70),\n",
       " 'valid hr ppm': np.int64(96),\n",
       " 'special select receiv': np.int64(81),\n",
       " 'speak live oper': np.int64(80),\n",
       " 'live oper claim': np.int64(45),\n",
       " 'privat account statement': np.int64(68),\n",
       " 'account statement show': np.int64(0),\n",
       " 'call identifi code': np.int64(7),\n",
       " 'identifi code expir': np.int64(40),\n",
       " 'bonu caller prize': np.int64(4),\n",
       " 'select receiv award': np.int64(76),\n",
       " 'match pleas call': np.int64(54),\n",
       " 'urgent tri contact': np.int64(95),\n",
       " 'lt decim gt': np.int64(47),\n",
       " 'secret admir look': np.int64(75),\n",
       " 'admir look make': np.int64(1),\n",
       " 'look make contact': np.int64(46),\n",
       " 'make contact find': np.int64(53),\n",
       " 'contact find reveal': np.int64(19),\n",
       " 'find reveal think': np.int64(28),\n",
       " 'reveal think ur': np.int64(73),\n",
       " 'think ur special': np.int64(85),\n",
       " 'ur special call': np.int64(92),\n",
       " 'draw txt music': np.int64(24),\n",
       " 'www ldew com': np.int64(99),\n",
       " 'anytim network min': np.int64(2),\n",
       " 'camcord repli call': np.int64(12),\n",
       " 'cant pick phone': np.int64(13),\n",
       " 'pick phone right': np.int64(63),\n",
       " 'phone right pl': np.int64(62),\n",
       " 'right pl send': np.int64(74),\n",
       " 'pl send messag': np.int64(64),\n",
       " 'hg suit land': np.int64(37),\n",
       " 'suit land row': np.int64(84),\n",
       " 'land row hl': np.int64(42),\n",
       " 'call mobileupd call': np.int64(9),\n",
       " 'mobileupd call optout': np.int64(57),\n",
       " 'lt gt min': np.int64(49),\n",
       " 'free entri weekli': np.int64(29),\n",
       " 'urgent mobil number': np.int64(94),\n",
       " 'mobil number award': np.int64(56),\n",
       " 'guarante call land': np.int64(34),\n",
       " 'call land line': np.int64(8),\n",
       " 'land line claim': np.int64(41),\n",
       " 'line claim valid': np.int64(44),\n",
       " 'claim valid hr': np.int64(16),\n",
       " 'lt gt th': np.int64(51),\n",
       " 'lt gt lt': np.int64(48),\n",
       " 'gt lt gt': np.int64(33),\n",
       " 'caller prize nd': np.int64(11),\n",
       " 'prize nd attempt': np.int64(71),\n",
       " 'nd attempt contact': np.int64(58),\n",
       " 'ur worth discount': np.int64(93),\n",
       " 'worth discount voucher': np.int64(97),\n",
       " 'statement show un': np.int64(82),\n",
       " 'show un redeem': np.int64(78),\n",
       " 'un redeem point': np.int64(90),\n",
       " 'redeem point call': np.int64(72),\n",
       " 'point call identifi': np.int64(67),\n",
       " 'new video phone': np.int64(59),\n",
       " 'happi new year': np.int64(36),\n",
       " 'everi wk txt': np.int64(27),\n",
       " 'lt gt minut': np.int64(50),\n",
       " 'free st week': np.int64(30),\n",
       " 'nokia tone ur': np.int64(60),\n",
       " 'everi week txt': np.int64(26),\n",
       " 'get txting tell': np.int64(31),\n",
       " 'www getz co': np.int64(98),\n",
       " 'getz co uk': np.int64(32),\n",
       " 'co uk pobox': np.int64(17),\n",
       " 'uk pobox wq': np.int64(89),\n",
       " 'pobox wq norm': np.int64(66),\n",
       " 'holiday cash await': np.int64(39),\n",
       " 'cash await collect': np.int64(14),\n",
       " 'await collect sae': np.int64(3),\n",
       " 'collect sae cs': np.int64(18),\n",
       " 'stop text call': np.int64(83),\n",
       " 'tri contact today': np.int64(88),\n",
       " 'contact today draw': np.int64(20),\n",
       " 'today draw show': np.int64(86),\n",
       " 'tone ur mob': np.int64(87),\n",
       " 'ur mob everi': np.int64(91),\n",
       " 'mob everi week': np.int64(55),\n",
       " 'call per min': np.int64(10),\n",
       " 'per min ntt': np.int64(61),\n",
       " 'ltd po box': np.int64(52),\n",
       " 'bt nation rate': np.int64(5),\n",
       " 'prize claim easi': np.int64(69),\n",
       " 'claim easi call': np.int64(15),\n",
       " 'easi call per': np.int64(25),\n",
       " 'costa del sol': np.int64(21),\n",
       " 'holiday await collect': np.int64(38)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a BOW \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#we are picking only 100 words which has high frequency\n",
    "cv=CountVectorizer(max_features=100,binary=True,ngram_range=(3,3)) #Var the range to get uni,bi,trigrams \n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "cv.vocabulary_\n",
    "                   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
