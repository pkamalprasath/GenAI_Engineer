{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713b2452",
   "metadata": {},
   "source": [
    "### TF-IDF [Term Frequency-Inverse Document Frequency]\n",
    "It's a clever mathematical trick used by computers to figure out which words are the most important and unique in a piece of text. TF (Term Frequency):- just counts the words. IDF :- is the smart one. It knows that some words are just too common to be important. TF-IDF is a score that tells the computer: \"This word appears a lot in this text, AND it's a rare word across all texts, which means it must be the main topic!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to import the file with sample data\n",
    "# use sep \\t to seperate the input data into lables and message \n",
    "# if pandas libraries are not present pip install that. \n",
    "\n",
    "import pandas as pd\n",
    "messages=pd.read_csv('D:/AI/KrishNaik_Academy/Coding/NLP/Text_Prepocessing/input/SMSSpamCollection',\n",
    "                     sep='\\t',names=[\"lables\",\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187bf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data cleaning and Preprocessing\n",
    "import re # importing regular expression lib\n",
    "import nltk # for text preprocessing \n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8526d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesinng -> \n",
    "#   ->Match the regex ,replace non alphabet with space \n",
    "#   ->convert that into lower case\n",
    "#   ->split the final word \n",
    "#   ->Join them into document and store is in list named corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus=[] # empty list \n",
    "for i in range(0,len(messages)):\n",
    "    #From re lib we are using \"sub\" function to match the regex \n",
    "    #replace that other character with space and store it in messages list \n",
    "    review=re.sub('[^a-zA-Z]',' ',messages['message'][i])  \n",
    "    review=review.lower()\n",
    "    review=review.split() \n",
    "    review=[lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english')) ] \n",
    "    review=' '.join(review)  \n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b72a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create TF-IDF and N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a BOW \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#we are picking only 100 words which has high frequency\n",
    "cv=TfidfVectorizer(max_features=100)\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To display the array in proper order \n",
    "import numpy as np \n",
    "np.set_printoptions(edgeitems=30,linewidth=100000,\n",
    "                    formatter=dict(float=lambda X: \"%.3g\" % X))\n",
    "#edgeitems=30 - Show a lot of numbers at the beginning and end \n",
    "#linewidth=100000 - Don't break the array into multiple lines \n",
    "#formatter= - Make the decimal numbers look neat \n",
    "# [%.3g - Show each number with at most three significant figures ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88808c",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(max_features=100,ngram_range=(3,3))\n",
    "X=tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ad0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b4de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
